{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing packages"
      ],
      "metadata": {
        "id": "hlxQfVu3AyRv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ8MxqYz3GsA"
      },
      "outputs": [],
      "source": [
        "!pip install lottery-ticket-pruner\n",
        "!pip install matplotlib\n",
        "!pip install --user torch\n",
        "!pip install pandas\n",
        "!pip install -q transformers\n",
        "!pip install --user tqdm\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Warning***: Depending on the runtime used, you might have to restart the kernel in order for the new libraries to be located properly."
      ],
      "metadata": {
        "id": "x7joWqNylLFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "wlxmrW0RA1n2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3g4Wgl63dgP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from lottery_ticket_pruner import LotteryTicketPruner\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification,AutoModel, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRdtHPNgATzv"
      },
      "source": [
        "Download tokenized tweets from google drive. If the automatic download fails, please copy-paste the link in the browser, download them, and upload them manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8R3idfEATzy"
      },
      "outputs": [],
      "source": [
        "files={\n",
        "    \"attention_masks.pkl\": \"https://drive.google.com/uc?id=1Th1ry1vnFkRsvjNU2s-bzLeeluHBIY7o\",\n",
        "    \"sentiments_encoded.pkl\": \"https://drive.google.com/uc?id=1pblp6oH4Uz125bZx5jcMK-Wq0Bf3TO0J\",\n",
        "    \"token_types.pkl\": \"https://drive.google.com/uc?id=1bKn1U9h83kLQQGwiKTDAKVLB_csqAG_J\",\n",
        "    \"tweets_encoded.pkl\": \"https://drive.google.com/uc?id=1ZjHOZhml728fz_xa-3AGg9wysTNqjC1K\",\n",
        "}\n",
        "for fname, url in files.items():\n",
        "    gdown.download(url, fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the tokenized tweets"
      ],
      "metadata": {
        "id": "8XdYSZXHA_BL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX2N3XPmATz0"
      },
      "outputs": [],
      "source": [
        "def load_from_pickle():\n",
        "    with open('tweets_encoded.pkl', 'rb') as f:\n",
        "        tweets_encoded = pickle.load(f)\n",
        "        print(\"Loaded tweets_encoded\")\n",
        "    with open('attention_masks.pkl', 'rb') as f:\n",
        "        attention_masks = pickle.load(f)\n",
        "        print(\"Loaded attention_masks\")\n",
        "    with open('token_types.pkl', 'rb') as f:\n",
        "        token_types = pickle.load(f)\n",
        "        print(\"Loaded token_types\")\n",
        "    with open('sentiments_encoded.pkl', 'rb') as f:\n",
        "        sentiments_encoded = pickle.load(f)\n",
        "        print(\"Loaded sentiments_encoded\")\n",
        "\n",
        "    return tweets_encoded, attention_masks, token_types, sentiments_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIF8nanvATz1"
      },
      "outputs": [],
      "source": [
        "tweets_encoded, attention_masks, token_types, sentiments_encoded = load_from_pickle()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, sentiments = [tweets_encoded, attention_masks, token_types], sentiments_encoded"
      ],
      "metadata": {
        "id": "PqVw_irBBhJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHYOkMZ-uZkF"
      },
      "source": [
        "# Define max length of embedding and the batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPpX7BzWXDqT"
      },
      "outputs": [],
      "source": [
        "no_gpu = 8\n",
        "max_length = 512 #of embedding\n",
        "batch_size = 40 * no_gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Fw-MRgjau1"
      },
      "source": [
        "# Helper function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9uWPrDQ2OTV"
      },
      "outputs": [],
      "source": [
        "def map_tweet_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "  }, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXmZ7MDDtsy5"
      },
      "source": [
        "# Make train, validation, and test data\n",
        "Here we have: 10% train, 5% validation, and 5% test due to time constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDIF_Uaun2g-"
      },
      "outputs": [],
      "source": [
        "def make_train_data(start_tweets, start_sentiments):\n",
        "  print(\"I start with:\"+str(len(start_sentiments)))\n",
        "  np.random.seed(33) \n",
        "  msk = np.random.rand(len(start_sentiments)) < 0.2\n",
        "  tweets = []\n",
        "  sentiments = []\n",
        "\n",
        "  a,b,c = [], [], []\n",
        "  for i in range(len(start_sentiments)):\n",
        "    if msk[i] == True:\n",
        "      a.append(start_tweets[0][i])\n",
        "      b.append(start_tweets[1][i])\n",
        "      c.append(start_tweets[2][i])\n",
        "      sentiments.append(start_sentiments[i])\n",
        "  tweets = [a,b,c]\n",
        "\n",
        "  print(\"I continue with:\"+str(len(sentiments))) \n",
        "  ratio = 0.5\n",
        "  pos, neg = sentiments[: sentiments.count(1)], sentiments[- sentiments.count(0):]\n",
        "  msk1 = np.random.rand(len(pos)) < ratio\n",
        "  msk2 = np.random.rand(len(neg)) < ratio\n",
        "\n",
        "  train_tweets = [[], [], []]\n",
        "  train_sentiments = []\n",
        "\n",
        "  validation_tweets = [[], [], []]\n",
        "  validation_sentiments = []\n",
        "\n",
        "  test_tweets = [[], [], []]\n",
        "  test_sentiments = []\n",
        "\n",
        "  rest_tweets = [[], [], []] \n",
        "  rest_sentiments = []\n",
        "\n",
        "  for i in range(len(pos)):\n",
        "    if msk1[i] == True:\n",
        "      train_tweets[0].append(tweets[0][i]) \n",
        "      train_tweets[1].append(tweets[1][i]) \n",
        "      train_tweets[2].append(tweets[2][i]) \n",
        "      train_sentiments.append(sentiments[i])\n",
        "    else:\n",
        "      rest_tweets[0].append(tweets[0][i]) \n",
        "      rest_tweets[1].append(tweets[1][i]) \n",
        "      rest_tweets[2].append(tweets[2][i]) \n",
        "      rest_sentiments.append(sentiments[i])\n",
        "        \n",
        "  for i in range(len(neg)):\n",
        "    if msk2[i] == True:\n",
        "      train_tweets[0].append(tweets[0][len(pos) + i])\n",
        "      train_tweets[1].append(tweets[1][len(pos) +i]) \n",
        "      train_tweets[2].append(tweets[2][len(pos) +i])  \n",
        "      train_sentiments.append(sentiments[len(pos) +i])\n",
        "    else:\n",
        "      rest_tweets[0].append(tweets[0][len(pos) +i])\n",
        "      rest_tweets[1].append(tweets[1][len(pos) +i])\n",
        "      rest_tweets[2].append(tweets[2][len(pos) +i]) \n",
        "      rest_sentiments.append(sentiments[len(pos) +i])\n",
        "\n",
        "\n",
        "\n",
        "  rest_ratio = 0.5\n",
        "  pos, neg = rest_sentiments[: rest_sentiments.count(1)], rest_sentiments[- rest_sentiments.count(0):]\n",
        "  msk1 = np.random.rand(len(pos)) < rest_ratio\n",
        "  msk2 = np.random.rand(len(neg)) < rest_ratio\n",
        "\n",
        "  \n",
        "  for i in range(len(pos)):\n",
        "    if msk1[i] == True:\n",
        "      validation_tweets[0].append(rest_tweets[0][i])\n",
        "      validation_tweets[1].append(rest_tweets[1][i])\n",
        "      validation_tweets[2].append(rest_tweets[2][i]) \n",
        "      validation_sentiments.append(rest_sentiments[i])\n",
        "    else:\n",
        "      test_tweets[0].append(rest_tweets[0][i]) \n",
        "      test_tweets[1].append(rest_tweets[1][i]) \n",
        "      test_tweets[2].append(rest_tweets[2][i]) \n",
        "      test_sentiments.append(rest_sentiments[i])\n",
        "\n",
        "  for i in range(len(neg)):\n",
        "    if msk2[i] == True:\n",
        "      validation_tweets[0].append(rest_tweets[0][len(pos) + i]) \n",
        "      validation_tweets[1].append(rest_tweets[1][len(pos) + i]) \n",
        "      validation_tweets[2].append(rest_tweets[2][len(pos) + i]) \n",
        "      validation_sentiments.append(rest_sentiments[len(pos) + i])\n",
        "    else:\n",
        "      test_tweets[0].append(rest_tweets[0][len(pos) + i]) \n",
        "      test_tweets[1].append(rest_tweets[1][len(pos) + i]) \n",
        "      test_tweets[2].append(rest_tweets[2][len(pos) + i]) \n",
        "      test_sentiments.append(rest_sentiments[len(pos) + i])\n",
        "\n",
        "  print(\"I have: Train:\"+str(len(train_sentiments)) + \" Validation:\" +str(len(validation_sentiments)) + \" Test:\" +str(len(test_sentiments))) \n",
        "\n",
        "  train_tweets_ds = tf.data.Dataset.from_tensor_slices((train_tweets[0], train_tweets[1], train_tweets[2], train_sentiments)).map(map_tweet_to_dict).shuffle(len(train_sentiments)).batch(batch_size)\n",
        "  print(\"Train loaded\")\n",
        "  validation_tweets_ds = tf.data.Dataset.from_tensor_slices((validation_tweets[0], validation_tweets[1],validation_tweets[2], validation_sentiments)).map(map_tweet_to_dict).shuffle(len(validation_sentiments)).batch(batch_size)\n",
        "  print(\"Validation loaded\")\n",
        "  test_tweets_ds = tf.data.Dataset.from_tensor_slices((test_tweets[0], test_tweets[1], test_tweets[2],test_sentiments)).map(map_tweet_to_dict).batch(batch_size)\n",
        "  print(\"Test loaded\")\n",
        "  return (train_tweets_ds, train_sentiments), (validation_tweets_ds, validation_sentiments), (test_tweets_ds, test_sentiments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbC0HjUCkq3q"
      },
      "source": [
        "# Define the BERT model and training procedure #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMz2UrOU3mi8"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Create the initial BERT model - from these weigths we start at each one-shot pruning trial and at the begining of iterative pruning\n",
        "def model_builder():\n",
        "  model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
        "  return model\n",
        "\n",
        "\n",
        "# Creates the model which had its weights pruned \n",
        "def return_model(model):\n",
        "    new_model = model_builder()\n",
        "    new_model.set_weights(model.get_weights())\n",
        "    return new_model\n",
        "\n",
        "\n",
        "# The same pruning strategy seen in previous notebooks. We also deploy early stopping for preventing overfitting.\n",
        "# Note that we save the epoch when training stopped due to early stopping so that we don't go past that when training the pruned networks - as required in LTH\n",
        "def train_model(modelX, num_epochs, X_train, Y_train, X_eval, Y_eval):\n",
        "    \n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience= 3, restore_best_weights= True);\n",
        "    gpus = tf.config.list_logical_devices('GPU')\n",
        "    if len(gpus) > 1:\n",
        "      strategy = tf.distribute.MirroredStrategy(gpus)\n",
        "      with strategy.scope():\n",
        "\n",
        "          model = return_model(modelX)\n",
        "\n",
        "          optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "          loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "          metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "          model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "    history = model.fit(X_train, batch_size= batch_size, epochs= num_epochs, validation_data=X_eval, callbacks=[stop_early, ])\n",
        "\n",
        "    best_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
        "    \n",
        "    return(model,best_epoch);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ5DypS90gjo"
      },
      "source": [
        "__For obtaining the class prediction__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaPy0cbV2Joa"
      },
      "outputs": [],
      "source": [
        "# Obtaining the class that was predicted for each tweet from the decisions list containing the softmaxes output\n",
        "def get_decision(decisions):\n",
        "  results = []\n",
        "  for i in range(len(decisions[0])):\n",
        "    results.append(np.argmax(decisions[0][i]))\n",
        "\n",
        "  return results "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2qmQ_i1liKn"
      },
      "source": [
        "# Evaluate the results on the test set#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Iifwm54h6a"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model/pruned on the test set: Compute the accuracy, TPR and TNR\n",
        "def evaluate(net,X,Y):\n",
        "    Z = net.predict(X,batch_size=10)\n",
        "    Zbin = get_decision(Z);\n",
        "\n",
        "    Y = np.array(Y)\n",
        "    Z = np.array(Z)\n",
        "    Zbin = np.array(Zbin)\n",
        "\n",
        "    #Compute the acc, tpr, tnr\n",
        "    n = len(Zbin); \n",
        "    n0 = np.sum(Y==0); \n",
        "    n1 = np.sum(Y==1);\n",
        "\n",
        "    acc = np.sum(Zbin == Y) / n;\n",
        "    tpr = np.sum(Zbin[Y==1]) / n1;\n",
        "    tnr = np.sum(Zbin[Y==0] == 0) / n0;\n",
        "\n",
        "    print(\"Acc: \" + str(acc)+ str(\"\\t\") + \n",
        "          \"Tpr:\" + str(tpr) + str(\"\\t\") +\n",
        "          \"Tnr:\" + str(tnr) + str(\"\\t\"));\n",
        "  \n",
        "    return(acc, tpr, tnr); "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0dWTS30l-Wy"
      },
      "source": [
        "# Create a plot with values for the baseline, pruned with one-shot, and pruned with iterative model #\n",
        "The values can be Accuracy, TPR, or TNR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKmPoVFKM3G7"
      },
      "outputs": [],
      "source": [
        "def create_figure(title, x_label, y_label, x0, y0, xy0err, x1, y1, xy1err, x2, y2, xy2err):\n",
        "    \n",
        "    fig, ax =  plt.subplots(figsize=(10,10));\n",
        "    \n",
        "    #trained baseline model \n",
        "    err= ax.errorbar(x0,y0,color = 'k', yerr=xy0err, ecolor='k', elinewidth=1, capsize=3);\n",
        "    err[-1][0].set_linestyle('--'); \n",
        "    \n",
        "    #one shot from initial\n",
        "    err=ax.errorbar(x1,y1, color ='r',yerr=xy1err, ecolor=\"r\", elinewidth=1, capsize=3);\n",
        "    err[-1][0].set_linestyle('--');\n",
        "    \n",
        "    #iterative from initial\n",
        "    err=ax.errorbar(x2,y2, color='b', yerr=xy2err, ecolor=\"b\", elinewidth=1, capsize=3);\n",
        "    err[-1][0].set_linestyle('--');\n",
        "  \n",
        "    plt.xlabel(x_label);\n",
        "    plt.ylabel(y_label);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WhkKhrCmBwp"
      },
      "source": [
        "# Prune a model using the one-shot or iterative pruning method for multiple values of *p%* and train it #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7EPQ-rSxdPC"
      },
      "outputs": [],
      "source": [
        "def apply_pruning_method(pruning_method, num_repetition, X_train, Y_train, X_eval, Y_eval, X_test, Y_test, num_epochs, num_pruning_trials, initial_baseline_model, initial_baseline_model_weights, trained_baseline_model, trained_baseline_model_weights, prune_strategy, prune_percentage_for_iterative):\n",
        "\n",
        "    #Instantiate the pruner\n",
        "    pruner = LotteryTicketPruner(initial_baseline_model);\n",
        "\n",
        "    #Store the accs, tprs, tnrs for each pruning trial (7 in total)\n",
        "    pruned_accs =[];\n",
        "    pruned_tprs = [];\n",
        "    pruned_tnrs = [];\n",
        "    \n",
        "    pruning_trial_no=0;\n",
        "    \n",
        "\n",
        "    #Conduct numtiple pruning trials \n",
        "    for prune_percentage in list(np.linspace(start=0.3,stop=1,num=num_pruning_trials+1))[0:-1]:\n",
        "        \n",
        "        pruning_trial_no= pruning_trial_no+1;\n",
        "        \n",
        "        #Decide from what mask you want to start (new one for one-shot, previous one for iterative)\n",
        "        if pruning_method==\"one_shot\":\n",
        "          pruner.reset_masks();\n",
        "        \n",
        "        elif pruning_method==\"iterative\":\n",
        "          prune_percentage = prune_percentage_for_iterative;\n",
        "          \n",
        "        #Set the initial weights and the ones after training    \n",
        "        initial_baseline_model.set_weights(initial_baseline_model_weights);\n",
        "        trained_baseline_model.set_weights(trained_baseline_model_weights);\n",
        "\n",
        "        #Here the mask is obtained by removing prune_percentage of the trained_baseline_model's weights\n",
        "        pruner.calc_prune_mask(trained_baseline_model, prune_percentage, prune_strategy);\n",
        "        \n",
        "        #Prune the initial_baseline_model of its weights using the above-computed mask (and starting from the same initial weights)\n",
        "        pruner.apply_pruning(initial_baseline_model);\n",
        "\n",
        "        \n",
        "        #Train the above-pruned initial_baseline_model for at most the same number of epochs as the trained_baseline_model\n",
        "        (pruned_trained_model, stoped_epoch) = train_model(initial_baseline_model, num_epochs, X_train, Y_train, X_eval, Y_eval);\n",
        "\n",
        "        print(pruning_method + \" Experiment repetition no:\"+ str(num_repetition)+ \" Pruned model at \" + str(prune_percentage) + \"pruning trial:\"+ str(pruning_trial_no)+\"\\n\");\n",
        "        \n",
        "        #Evaluate the pruned_trained_model\n",
        "        (acc, tpr, tnr) = evaluate(pruned_trained_model, X_test, Y_test);\n",
        "\n",
        "        pruned_accs.append(acc);\n",
        "        pruned_tprs.append(tpr);\n",
        "        pruned_tnrs.append(tnr);\n",
        "\n",
        "    #Return accs, tprs, tnrs for the (7) pruning trials\n",
        "    print(pruning_method + \" After pruning at all percentages:\")\n",
        "    print(\"Accs:\"+ str(pruned_accs))\n",
        "    print(\"TPRS:\"+ str(pruned_tprs))\n",
        "    print(\"TNRS:\"+ str(pruned_tnrs))\n",
        "    return(pruned_accs, pruned_tprs, pruned_tnrs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12FCecJDmjSL"
      },
      "source": [
        "# Repeat:  Prune a model using the one-shot or iterative pruning method for multiple values of *p%*, train it, and evaluate the results #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X3G4E6sVMVT"
      },
      "outputs": [],
      "source": [
        "def repeat_experiment(num_epochs=20, num_experiment_repetitions=5, num_pruning_trials= 9, prune_strategy=\"smallest_weights_global\", prune_percentage_for_iterative=0.2):\n",
        "    \n",
        "    \n",
        "    #Accs, tprs, tnrs for all (3) repetitions and (7) pruning trials for the baseline, pruned with one-shot and pruned with iterative models\n",
        "    baseline_accuracies_all_trials=[];\n",
        "    baseline_tprs_all_trials=[];\n",
        "    baseline_tnrs_all_trials=[];\n",
        "\n",
        "    pruned_accuracies_all_trials_one_shot_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "    pruned_tprs_all_trials_one_shot_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "    pruned_tnrs_all_trials_one_shot_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "\n",
        "    pruned_accuracies_all_trials_iterative_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "    pruned_tprs_all_trials_iterative_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "    pruned_tnrs_all_trials_iterative_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "\n",
        "   \n",
        "    #Repeat the experiment multiple times 3\n",
        "    for repetition in range(0,num_experiment_repetitions):\n",
        "        print(\"****************\")\n",
        "        print(\"Experiment repetition:\" + str(repetition))\n",
        "        print(\"****************\")\n",
        "\n",
        "        #Compute the dataset \n",
        "        (X_train, Y_train), (X_eval, Y_eval), (X_test, Y_test) = make_train_data(tweets, sentiments)\n",
        "        print(\"Dataset computed\")\n",
        "        \n",
        "        #Build the initial_baseline_model and save its weights   \n",
        "        initial_baseline_model = model_builder();\n",
        "        initial_baseline_model_weights = initial_baseline_model.get_weights();\n",
        "\n",
        "        #Train the initial_baseline_model and save the trained model's weights   \n",
        "        (trained_baseline_model,trnd_bm_stop_epoch) = train_model(initial_baseline_model, num_epochs, X_train, Y_train, X_eval, Y_eval);\n",
        "        trained_baseline_model_weights = trained_baseline_model.get_weights();\n",
        "       \n",
        "        #Evaluate the trained_baseline_model \n",
        "        print(\"-------- Baseline model results at repetition \"+str(repetition)+\":\\n\");\n",
        "        (b_acc, b_tpr, b_tnr) =evaluate(trained_baseline_model, X_test, Y_test);\n",
        "        \n",
        "        #Save the trained_baseline_model's acc, tpr, tnr\n",
        "        baseline_accuracies_all_trials.append(b_acc);\n",
        "        baseline_tprs_all_trials.append(b_tpr);\n",
        "        baseline_tnrs_all_trials.append(b_tnr);\n",
        "\n",
        "        #Conduct one-shot pruning one time, for multiple pruning percentages (7 trials) \n",
        "        print(\"-------- Starting One-Shot with initialization from original initial network:\\n\");\n",
        "        (oneShot_from_initial_pruned_accs, oneShot_from_initial_pruned_tprs, oneShot_from_initial_pruned_tnrs)=apply_pruning_method(\"one_shot\", repetition, X_train, Y_train, X_eval, Y_eval, X_test, Y_test, trnd_bm_stop_epoch, num_pruning_trials, initial_baseline_model, initial_baseline_model_weights, trained_baseline_model, trained_baseline_model_weights, prune_strategy, prune_percentage_for_iterative);\n",
        "        \n",
        "        #Conduct iterative pruning one time, for multiple pruning percentages (7 trials) \n",
        "        print(\"-------- Starting Iterative with initialization from original initial network:\\n\");\n",
        "        (iterative_from_initial_pruned_accs, iterative_from_initial_pruned_tprs, iterative_from_initial_pruned_tnrs)=apply_pruning_method(\"iterative\", repetition, X_train, Y_train, X_eval, Y_eval, X_test, Y_test, trnd_bm_stop_epoch, num_pruning_trials, initial_baseline_model, initial_baseline_model_weights, trained_baseline_model, trained_baseline_model_weights, prune_strategy, prune_percentage_for_iterative);\n",
        "  \n",
        "        #Save the accs, tprs, tnrs from the above 3*7 pruning trials\n",
        "        for i in range(0,num_pruning_trials):\n",
        "            pruned_accuracies_all_trials_one_shot_from_initial[i].append(oneShot_from_initial_pruned_accs[i]);\n",
        "            pruned_tprs_all_trials_one_shot_from_initial[i].append(oneShot_from_initial_pruned_tprs[i]);\n",
        "            pruned_tnrs_all_trials_one_shot_from_initial[i].append(oneShot_from_initial_pruned_tnrs[i]);\n",
        "\n",
        "            pruned_accuracies_all_trials_iterative_from_initial[i].append(iterative_from_initial_pruned_accs[i]);\n",
        "            pruned_tprs_all_trials_iterative_from_initial[i].append(iterative_from_initial_pruned_tprs[i]);\n",
        "            pruned_tnrs_all_trials_iterative_from_initial[i].append(iterative_from_initial_pruned_tnrs[i]);\n",
        "            \n",
        "    #Return the accs, tprs, tnrs for the baseline, pruned with one-shot, pruned with iterative models.    \n",
        "    return (    baseline_accuracies_all_trials, baseline_tprs_all_trials, baseline_tnrs_all_trials,\n",
        "                pruned_accuracies_all_trials_one_shot_from_initial, pruned_tprs_all_trials_one_shot_from_initial, pruned_tnrs_all_trials_one_shot_from_initial, \n",
        "                pruned_accuracies_all_trials_iterative_from_initial, pruned_tprs_all_trials_iterative_from_initial, pruned_tnrs_all_trials_iterative_from_initial,\n",
        "           );\n",
        "            \n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqz7c8bRnHmy"
      },
      "source": [
        "Compute pruning percentage (*p%*) values for iterative pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlTbq2oYVenT"
      },
      "outputs": [],
      "source": [
        "def compute_pruning_percentages_iterative(num_pruning_trials, pruning_percentage):\n",
        "  \n",
        "  remaining_weights=1;\n",
        "  percentages=[];\n",
        "\n",
        "  for i in range(0, num_pruning_trials):\n",
        "    remaining_weights= remaining_weights - remaining_weights*pruning_percentage;\n",
        "    percentages.append(1- remaining_weights);\n",
        "\n",
        "  return percentages;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3E0VyE3nSOW"
      },
      "source": [
        "# Create the Accuracy, TPR and TNR plots #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLDsMtOYHQg1"
      },
      "outputs": [],
      "source": [
        "def create_plots_for_executed_experiments(num_experiment_repetitions, num_pruning_trials, prune_percentage_for_iterative, b_accs, b_tprs, b_tnrs, os_init_accs, os_init_tprs, os_init_tnrs, it_init_accs, it_init_tprs, it_init_tnrs):\n",
        "\n",
        "      #Compute pruning percentages for both experiments (7 pruning percentages)  \n",
        "      pruning_percentages_one_shot = list(np.linspace(start=0.3,stop=1,num=num_pruning_trials+1))[0:-1];\n",
        "      pruning_percentages_iterative = compute_pruning_percentages_iterative(num_pruning_trials, prune_percentage_for_iterative);\n",
        "\n",
        "      #Store the accs, tprs, tnrs per experiment repetition\n",
        "      b_accs0 =[];\n",
        "      b_accs0err=[[],[]];\n",
        "      b_tprs0 =[];\n",
        "      b_tprs0err = [[],[]];\n",
        "      b_tnrs0 = [];\n",
        "      b_tnrs0err = [[],[]]; \n",
        "        \n",
        "    \n",
        "      os_init_accs1= [];\n",
        "      os_init_accs1err= [[],[]];\n",
        "\n",
        "      os_init_tprs1= [];\n",
        "      os_init_tprs1err= [[],[]];\n",
        "\n",
        "      os_init_tnrs1= [];\n",
        "      os_init_tnrs1err= [[],[]];\n",
        "      \n",
        "\n",
        "      it_init_accs3= [];\n",
        "      it_init_accs3err= [[],[]];\n",
        "\n",
        "      it_init_tprs3= [];\n",
        "      it_init_tprs3err= [[],[]];\n",
        "\n",
        "      it_init_tnrs3= [];\n",
        "      it_init_tnrs3err= [[],[]];\n",
        "      \n",
        "      #Put the accs, tprs, tnrs and error bars in the corresponding lists for making the plots for the baseline model  \n",
        "      b_accs0 = [np.mean(b_accs)] *  num_pruning_trials;\n",
        "      b_accs0err[0] = [np.mean(b_accs)-np.min(b_accs)] * num_pruning_trials;\n",
        "      b_accs0err[1] = [np.max(b_accs) -np.mean(b_accs)] * num_pruning_trials;\n",
        "\n",
        "      b_tprs0 = [np.mean(b_tprs)] * num_pruning_trials;\n",
        "      b_tprs0err[0] = [np.mean(b_tprs)-np.min(b_tprs)]* num_pruning_trials;\n",
        "      b_tprs0err[1] = [np.max(b_tprs) -np.mean(b_tprs)]* num_pruning_trials;\n",
        "\n",
        "      b_tnrs0 = [np.mean(b_tnrs)] * num_pruning_trials;\n",
        "      b_tnrs0err[0] = [np.mean(b_tnrs)-np.min(b_tnrs)] * num_pruning_trials;\n",
        "      b_tnrs0err[1] = [np.max(b_tnrs)-np.mean(b_tnrs)] * num_pruning_trials;\n",
        "\n",
        "\n",
        "\n",
        "      #Put the accs, tprs, tnrs and error bars in the corresponding lists for making the plots for the pruned with one-shot and iterative model\n",
        "      for repetition in range(0, num_pruning_trials):\n",
        "        \n",
        "          os_init_accs1.append(np.mean(os_init_accs[repetition]));\n",
        "          os_init_accs1err[0].append(np.mean(os_init_accs[repetition])- np.min(os_init_accs[repetition]));\n",
        "          os_init_accs1err[1].append(np.max(os_init_accs[repetition]) - np.mean(os_init_accs[repetition]));\n",
        "\n",
        "          os_init_tprs1.append(np.mean(os_init_tprs[repetition]));\n",
        "          os_init_tprs1err[0].append(np.mean(os_init_tprs[repetition]) - np.min(os_init_tprs[repetition]));\n",
        "          os_init_tprs1err[1].append(np.max(os_init_tprs[repetition]) - np.mean(os_init_tprs[repetition]));\n",
        "\n",
        "          os_init_tnrs1.append(np.mean(os_init_tnrs[repetition]));\n",
        "          os_init_tnrs1err[0].append(np.mean(os_init_tnrs[repetition])- np.min(os_init_tnrs[repetition]));\n",
        "          os_init_tnrs1err[1].append(np.max(os_init_tnrs[repetition]) -np.mean(os_init_tnrs[repetition]));\n",
        "        \n",
        "\n",
        "          it_init_accs3.append(np.mean(it_init_accs[repetition]));\n",
        "          it_init_accs3err[0].append(np.mean(it_init_accs[repetition]) - np.min(it_init_accs[repetition]));\n",
        "          it_init_accs3err[1].append(np.max(it_init_accs[repetition]) - np.mean(it_init_accs[repetition]));\n",
        "\n",
        "          it_init_tprs3.append(np.mean(it_init_tprs[repetition]));\n",
        "          it_init_tprs3err[0].append(np.mean(it_init_tprs[repetition])-np.min(it_init_tprs[repetition]));\n",
        "          it_init_tprs3err[1].append(np.max(it_init_tprs[repetition]) - np.mean(it_init_tprs[repetition]));\n",
        "\n",
        "          it_init_tnrs3.append(np.mean(it_init_tnrs[repetition]));\n",
        "          it_init_tnrs3err[0].append(np.mean(it_init_tnrs[repetition])-np.min(it_init_tnrs[repetition]));\n",
        "          it_init_tnrs3err[1].append(np.max(it_init_tnrs[repetition]) - np.mean(it_init_tnrs[repetition]));\n",
        "\n",
        "\n",
        "   \n",
        "      #Create the acc, tpr, tnr figures for the baseline, pruned with one-shot and iterative models\n",
        "      create_figure(\"Accuracies\",\"Pruned ratio\", \"Accuracy\", pruning_percentages_one_shot, b_accs0, b_accs0err, pruning_percentages_one_shot, os_init_accs1, os_init_accs1err, pruning_percentages_iterative, it_init_accs3, it_init_accs3err);\n",
        "      create_figure(\"TPRS\",\"Pruned ratio\", \"TPR\", pruning_percentages_one_shot, b_tprs0, b_tprs0err, pruning_percentages_one_shot, os_init_tprs1, os_init_tprs1err, pruning_percentages_iterative, it_init_tprs3, it_init_tprs3err);\n",
        "      create_figure(\"TNRS\",\"Pruned ratio\", \"TNR\", pruning_percentages_one_shot, b_tnrs0, b_tnrs0err, pruning_percentages_one_shot, os_init_tnrs1, os_init_tnrs1err, pruning_percentages_iterative, it_init_tnrs3, it_init_tnrs3err);\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLUh_AOKni9d"
      },
      "source": [
        "# Run the experiment #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6rq81yHruFy"
      },
      "outputs": [],
      "source": [
        "num_epochs=10;\n",
        "num_experiment_repetitions=3;\n",
        "num_pruning_trials= 7;\n",
        "\n",
        "prune_strategy=\"smallest_weights_global\";\n",
        "prune_percentage_for_iterative=0.3;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdqzYEwsrkA2"
      },
      "outputs": [],
      "source": [
        "(   b_accs, b_tprs, b_tnrs,\n",
        "    os_init_accs, os_init_tprs, os_init_tnrs,\n",
        "    it_init_accs, it_init_tprs, it_init_tnrs) = repeat_experiment(num_epochs, num_experiment_repetitions, num_pruning_trials,prune_strategy, prune_percentage_for_iterative);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye5dIpg4QORm"
      },
      "source": [
        "# Create the plots #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVSVmi9yq2qj"
      },
      "outputs": [],
      "source": [
        "create_plots_for_executed_experiments(num_experiment_repetitions, num_pruning_trials, prune_percentage_for_iterative, b_accs, b_tprs, b_tnrs, os_init_accs, os_init_tprs, os_init_tnrs, it_init_accs, it_init_tprs, it_init_tnrs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}