{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LXxx_SIplObJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing packages"
      ],
      "metadata": {
        "id": "IPoeYtwshKBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -q nltk\n",
        "!pip install --user -q ekphrasis\n",
        "!apt install swig3.0 -yqq\n",
        "!pip install --user -q jamspell\n",
        "!pip install --user -q gdown"
      ],
      "metadata": {
        "id": "kFzP6MPxdn4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Warning***: Depending on the runtime used, you might have to restart the kernel in order for the new libraries to be located properly."
      ],
      "metadata": {
        "id": "U-XHr5lMiFlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "_R8RLGDTVK60"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgCbHORbR7HU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from ekphrasis.dicts.noslang.slangdict import slangdict\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import jamspell\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading helper datasets needed for processing the original dataset\n"
      ],
      "metadata": {
        "id": "5UB1A_dEeHyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "hxdYnjifkuHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading pretrained spellchecker\n",
        "!wget https://github.com/bakwc/JamSpell-models/raw/master/en.tar.gz && tar -xvzf en.tar.gz"
      ],
      "metadata": {
        "id": "4Kei8A-Bylps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p misspellings\n",
        "for i in ['holbrook-missp.dat', 'aspell.dat', 'wikipedia.dat']:\n",
        "    !wget https://www.dcs.bbk.ac.uk/~ROGER/{i} -P misspellings -O misspellings/{i}"
      ],
      "metadata": {
        "id": "RkHPT3Y-SQP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "#Downloading extra_slang.json\n",
        "id = \"1eAYPSFxd6GjRstcWG9D4dnO_xbwfm2bZ\"\n",
        "url = f\"https://drive.google.com/uc?id={id}\"\n",
        "gdown.download(url)"
      ],
      "metadata": {
        "id": "B5ASu-JwkQfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Misspellings"
      ],
      "metadata": {
        "id": "ihL9mHIJaMP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct dictionary with the misspelling as key and the correct word as value from a dat file\n",
        "def dat2dict(file):\n",
        "    misp_dict = {}\n",
        "    with open(file) as f:\n",
        "        key, val = None, None\n",
        "        for index, line in enumerate(f):\n",
        "            term = line.rstrip().split(\" \")[0]\n",
        "            if \"_\" in term or \"'\" in term:\n",
        "                pass\n",
        "            if term.startswith('$'):\n",
        "                val = term[1:].lower()\n",
        "            else:\n",
        "                if len(term) > 3: # ignore small typos as they are hard to check\n",
        "                    key = term.lower()\n",
        "            if key and val:\n",
        "                misp_dict[key] = val\n",
        "    return misp_dict\n",
        "\n",
        "# Combines two dictionaries to form full dictionary for misspelings \n",
        "def merge_two_dicts(x, y):\n",
        "    z = x.copy()   \n",
        "    z.update(y)    \n",
        "    return z"
      ],
      "metadata": {
        "id": "K1q5gjRjT4Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the misspeling dictionaries\n",
        "miss_dict = {}\n",
        "if Path(\"roger_misp.pkl\").exists():\n",
        "    with open(\"roger_misp.pkl\", \"rb\") as f:\n",
        "        miss_dict = pickle.load(f)\n",
        "else:\n",
        "    for dat in Path(\"misspellings\").glob(\"*.dat\"):\n",
        "        miss_dict = merge_two_dicts(miss_dict, dat2dict(dat))\n",
        "    pickle.dump(miss_dict,open(\"roger_misp.pkl\",\"wb\"))"
      ],
      "metadata": {
        "id": "VbBDnw-vjCK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replacement dicts\n",
        "Load and process the dictionaries used to replace slang, emojis and contractions"
      ],
      "metadata": {
        "id": "LXxx_SIplObJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emojis"
      ],
      "metadata": {
        "id": "VL6CvagSltDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emojis = {\n",
        "    ':*': '<kiss>',\n",
        "    ':-*': '<kiss>',\n",
        "    ':x': '<kiss>',\n",
        "    ':-)': '<happy>',\n",
        "    ':-))': '<happy>',\n",
        "    ':)': '<happy>',\n",
        "    ':))': '<happy>',\n",
        "    ':o)': '<happy>',\n",
        "    ':]': '<happy>',\n",
        "    ':->': '<happy>',\n",
        "    ':>': '<happy>',\n",
        "    '8-)': '<happy>',\n",
        "    '8)': '<happy>',\n",
        "    ':-}': '<happy>',\n",
        "    ':o)': '<happy>',\n",
        "    ':-]': '<happy>',\n",
        "    ':3': '<happy>',\n",
        "    ':-3': '<happy>',\n",
        "    ':c)': '<happy>',\n",
        "    ':>': '<happy>',\n",
        "    '=]': '<happy>',\n",
        "    '8)': '<happy>',\n",
        "    '=)': '<happy>',\n",
        "    ':}': '<happy>',\n",
        "    ':^)': '<happy>',\n",
        "    '|;-)': '<happy>',\n",
        "    \":'-)\": '<happy>',\n",
        "    \":')\": '<happy>',\n",
        "    '\\o/': '<happy>',\n",
        "    '^^': '<happy>',\n",
        "    '^_^': '<happy>',\n",
        "    '*\\\\0/*': '<happy>',\n",
        "    ':-D': '<laugh>',\n",
        "    ':D': '<laugh>',\n",
        "    '8-D': '<laugh>',\n",
        "    '8D': '<laugh>',\n",
        "    'x-D': '<laugh>',\n",
        "    'xD': '<laugh>',\n",
        "    'X-D': '<laugh>',\n",
        "    'XD': '<laugh>',\n",
        "    '=-D': '<laugh>',\n",
        "    '=D': '<laugh>',\n",
        "    '=-3': '<laugh>',\n",
        "    '=3': '<laugh>',\n",
        "    'B^D': '<laugh>',\n",
        "    \"lool\": '<laugh>',\n",
        "    \"lol\": '<laugh>',\n",
        "    '>:[': '<sad>',\n",
        "    ':-(': '<sad>',\n",
        "    ':-((': '<sad>',\n",
        "    ':(': '<sad>',\n",
        "    ':((': '<sad>',\n",
        "    ':-c': '<sad>',\n",
        "    ':c': '<sad>',\n",
        "    ':-<': '<sad>',\n",
        "    ':<': '<sad>',\n",
        "    ':-[': '<sad>',\n",
        "    ':[': '<sad>',\n",
        "    ':{': '<sad>',\n",
        "    ':-||': '<sad>',\n",
        "    ':@': '<sad>',\n",
        "    \":'-(\": '<sad>',\n",
        "    \":'(\": '<sad>',\n",
        "    'D:<': '<sad>',\n",
        "    'D:': '<sad>',\n",
        "    'D8': '<sad>',\n",
        "    'D;': '<sad>',\n",
        "    'D=': '<sad>',\n",
        "    'DX': '<sad>',\n",
        "    'v.v': '<sad>',\n",
        "    \"D-':\": '<sad>',\n",
        "    '(>_<)': '<sad>',\n",
        "    ':|': '<sad>',\n",
        "    '>:O': '<surprise>',\n",
        "    ':-O': '<surprise>',\n",
        "    ':-o': '<surprise>',\n",
        "    ':O': '<surprise>',\n",
        "    '째o째': '<surprise>',\n",
        "    'o_O': '<surprise>',\n",
        "    'o_0': '<surprise>',\n",
        "    'o.O': '<surprise>',\n",
        "    'o-o': '<surprise>',\n",
        "    '8-0': '<surprise>',\n",
        "    '|-O': '<surprise>',\n",
        "    ';-)': '<wink>',\n",
        "    ';)': '<wink>',\n",
        "    '*-)': '<wink>',\n",
        "    '*)': '<wink>',\n",
        "    ';-]': '<wink>',\n",
        "    ';]': '<wink>',\n",
        "    ';D': '<wink>',\n",
        "    ';^)': '<wink>',\n",
        "    ':-,': '<wink>',\n",
        "    '>:P': '<tong>',\n",
        "    ':-P': '<tong>',\n",
        "    ':P': '<tong>',\n",
        "    'X-P': '<tong>',\n",
        "    'x-p': '<tong>',\n",
        "    'xp': '<tong>',\n",
        "    'XP': '<tong>',\n",
        "    ':-p': '<tong>',\n",
        "    ':p': '<tong>',\n",
        "    '=p': '<tong>',\n",
        "    ':-횧': '<tong>',\n",
        "    ':횧': '<tong>',\n",
        "    ':-b': '<tong>',\n",
        "    ':b': '<tong>',\n",
        "    ':-&': '<tong>',\n",
        "    '>:\\\\': '<annoyed>',\n",
        "    '>:/': '<annoyed>',\n",
        "    ':-/': '<annoyed>',\n",
        "    ':-.': '<annoyed>',\n",
        "    ':/': '<annoyed>',\n",
        "    ':\\\\': '<annoyed>',\n",
        "    '=/': '<annoyed>',\n",
        "    '=\\\\': '<annoyed>',\n",
        "    ':L': '<annoyed>',\n",
        "    '=L': '<annoyed>',\n",
        "    ':S': '<annoyed>',\n",
        "    '>.<': '<annoyed>',\n",
        "    ':-|': '<annoyed>',\n",
        "    '<:-|': '<annoyed>',\n",
        "    ':-X': '<seallips>',\n",
        "    ':X': '<seallips>',\n",
        "    ':-#': '<seallips>',\n",
        "    ':#': '<seallips>',\n",
        "    'O:-)': '<angel>',\n",
        "    '0:-3': '<angel>',\n",
        "    '0:3': '<angel>',\n",
        "    '0:-)': '<angel>',\n",
        "    '0:)': '<angel>',\n",
        "    '0;^)': '<angel>',\n",
        "    '>:)': '<devil>',\n",
        "    '>:D': '<devil>',\n",
        "    '>:-D': '<devil>',\n",
        "    '>;)': '<devil>',\n",
        "    '>:-)': '<devil>',\n",
        "    '}:-)': '<devil>',\n",
        "    '}:)': '<devil>',\n",
        "    '3:-)': '<devil>',\n",
        "    '3:)': '<devil>',\n",
        "    'o/\\o': '<highfive>',\n",
        "    '^5': '<highfive>',\n",
        "    '>_>^': '<highfive>',\n",
        "    '^<_<': '<highfive>',  \n",
        "    '<3': '<heart>',\n",
        "    'xx': 'kiss'\n",
        "}"
      ],
      "metadata": {
        "id": "UpLLJ2bMgno7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nd = {}\n",
        "for k,v in emojis.items():\n",
        "    # create lowercased version of all emoticons\n",
        "    if k.lower() not in emojis:\n",
        "        nd[k.lower()] = v\n",
        "\n",
        "emojis = merge_two_dicts(emojis, nd)\n",
        "emojis2 = {}\n",
        "#remove angle bracks from tags <heart> -> heart\n",
        "for k, v in emojis.items():\n",
        "    v = v.replace(\">\", \"\").replace(\"<\",'')\n",
        "    emojis2[k]=v\n",
        "emojis = emojis2"
      ],
      "metadata": {
        "id": "wtQNGS4PRlm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slang"
      ],
      "metadata": {
        "id": "Bb-6Xeqql6I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweak the original slang dictionary from nltk to fix some issues encountered on out dataset"
      ],
      "metadata": {
        "id": "FqM6SSt3mc04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slangdict.pop('im', None)\n",
        "slangdict.pop('blog', None)\n",
        "slangdict['dm'] = 'direct message'\n",
        "slangdict['ed'] = 'stupid'\n",
        "slangdict['x'] = 'kisses'\n",
        "slangdict['urg'] = 'pissed'\n",
        "slangdict['aww'] = 'affection'\n",
        "slangdict['aw'] = 'affection'\n",
        "slangdict['tf'] = 'what the fuck'\n",
        "slangdict['r'] = 'are'\n",
        "slangdict['ed'] = 'stupid'\n",
        "slangdict['wah'] = 'wonder'\n",
        "slangdict['tryna']= 'trying to'\n",
        "slangdict['xo'] = 'kisses'\n",
        "slangdict['frikken'] = 'freaking'\n",
        "slangdict['kn0w'] = 'know'\n",
        "slangdict['rt'] = ''"
      ],
      "metadata": {
        "id": "HSDuVEtuualD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load extra slang mappings"
      ],
      "metadata": {
        "id": "C0iQG0-Eme6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extra_slang = json.load(open('extra_slang.json'))\n",
        "extra_slang = dict((k.lower(), v.lower()) for k, v in extra_slang.items())"
      ],
      "metadata": {
        "id": "jwhLlbKI-gNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contractions"
      ],
      "metadata": {
        "id": "_gmdLhPtmi88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_cont = {\n",
        "    '/s': 'sarcasm',\n",
        "    'rt': '',\n",
        "    '2': 'to',\n",
        "    'u': 'you',\n",
        "    'wi-fi': 'wifi',\n",
        "    'im': \"i am\",\n",
        "    'bea': 'be a',\n",
        "    'taks': 'tasks',\n",
        "    'havea': 'have a',\n",
        "    \"dont\": \"do not\", \n",
        "    \"don't\": \"do not\",\n",
        "    \"doesnt\": \"does not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didnt\": \"did not\", \n",
        "    \"didn't\": \"did not\",\n",
        "    \"aint\": \"am not\",\n",
        "    \"ain't\": \"am not\",\n",
        "    \"arent\": \"are not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"isnt\": \"is not\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"wasnt\": \"was not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"havent\": \"have not\",\n",
        "    \"hasnt\": \"has not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadnt\": \"had not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"wont\": \"will not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldnt\": \"would not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"cant\": \"can not\",\n",
        "    \"can't\": \"can not\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldnt\": \"could not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"shant\": \"shall not\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldnt\": \"should not\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightnt\": \"might not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtnt\": \"ought not\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"daren't\": \"dare not\",\n",
        "    \"dang\": \"frustration\",\n",
        "    \"dangg\": \"frustration\",\n",
        "    \"ohh\": \"oh\",\n",
        "    \"mee\": \"me\",\n",
        "    \"ed\": \"stupid\",\n",
        "    \"domt\": \"do not\",\n",
        "    \"xo\": \"kisses\"\n",
        "}"
      ],
      "metadata": {
        "id": "BqfyH_FbrkHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text processing"
      ],
      "metadata": {
        "id": "scxnNIrkmr-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seg= Segmenter(\"twitter\")\n",
        "lm = nltk.WordNetLemmatizer()\n",
        "sm = EnglishStemmer()\n",
        "\n",
        "def lemmatizer(data): # For processing, lemmatization is used\n",
        "    return [lm.lemmatize(w) for w in data]\n",
        "\n",
        "def stemmatizer(data): #Not used \n",
        "    return [sm.stem(w) for w in data]"
      ],
      "metadata": {
        "id": "Z1ImN44Mi2Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "KcK16HvCmvWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_numbers(text):\n",
        "    return re.sub('\\w*[0-9]+', '', text)\n",
        "\n",
        "def replace_question(text):\n",
        "    return re.sub(r'(\\?)+', '', text)\n",
        "\n",
        "def replace_exclamation(text):\n",
        "    return re.sub(r'(\\!)+', '', text)\n",
        "\n",
        "def remove_punct(text):\n",
        "    punctionation = string.punctuation\n",
        "    text  = ''.join([char for char in text if char not in punctionation])\n",
        "    return text"
      ],
      "metadata": {
        "id": "5Xvdx5V6m7M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unpack_hashtag(text):\n",
        "    #Attempts to split a hashtag in segments\n",
        "    words = text.split()\n",
        "    return ' '.join([seg.segment(w[1:]) if (w[0] == '#') else w for w in words ])\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    #Returns a text without any stop words\n",
        "    #List of stop words is provided from nltk.corpus\n",
        "    text= text.lower()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_sentence = ' '.join([w for w in text.split() if not w in stop_words])\n",
        "    return filtered_sentence\n",
        "\n",
        "def handle_elong(text):\n",
        "    #Prevent enlongations by allowing a character to repeat up to 2 times only.\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "\n",
        "\n",
        "def handle_emoticons(text):\n",
        "    #Replace laugh expressions such as \"haha\",\"hihi\" and \"hehe\" by 'laugh'\n",
        "    #Replace emojis with explicit meaning from the dictionary\n",
        "    text = re.sub('(h+ah+a*h*)+', \"laugh\", text)\n",
        "    text = re.sub('(h*eh+e*h*)+', \"laugh\", text)\n",
        "    text= re.sub('(h*ih+i*h*)+', \"laugh\", text)\n",
        "    return ' '.join(emojis[w] if w in emojis else w for w in text.split())"
      ],
      "metadata": {
        "id": "_AWo3wuchruH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rm_slang(text):\n",
        "    #Use the slang dict form ekphrasis to replace slang\n",
        "    return ' '.join(slangdict[w] if w in slangdict else w for w in text.split())\n",
        "\n",
        "def rm_extra_slang(text):\n",
        "    #Use the extra slang dict to replace slang\n",
        "    return ' '.join(extra_slang[w] if w in extra_slang else w for w in text.split())"
      ],
      "metadata": {
        "id": "dWXMhH4untKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negations = {'no': 'NOT', 'not': \"NOT\", 'none': \"NOT_one\", 'nobody': \"NOT_body\", \n",
        "            'noone': 'NOT_one', 'nothing': 'NOT_thing', 'nowhere': \"NOT_where\", \n",
        "             'never': \"NOT_ever\"}\n",
        "\n",
        "def replace_negations(text):\n",
        "    stext = []\n",
        "    gets_prefix = False\n",
        "    for t in text.split():\n",
        "        negt = negations[t] if t in negations else \"\"\n",
        "        if gets_prefix:\n",
        "            t = \"NOT_\"+t\n",
        "            gets_prefix=False\n",
        "            stext.append(t)\n",
        "            continue\n",
        "        if negt == \"NOT\":\n",
        "            gets_prefix = True\n",
        "            continue\n",
        "        elif len(negt) > 0:\n",
        "            t = negt\n",
        "        stext.append(t)\n",
        "    return \" \".join(stext)"
      ],
      "metadata": {
        "id": "551qRcZrBU0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = set()\n",
        "\n",
        "#Remove tags and any ambigous 2 letter words remaining\n",
        "def remove_tags(tweet):\n",
        "    tw = []\n",
        "    for w in tweet.split(\" \"):\n",
        "        if \"<\" in w or len(w) < 3:\n",
        "            tags.add(w)\n",
        "        else:\n",
        "            tw.append(w)\n",
        "    return \" \".join(tw)"
      ],
      "metadata": {
        "id": "nmQc-dRMVxkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Social Tokenizer & Spellchecker"
      ],
      "metadata": {
        "id": "SE4SAINUpFhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Taken from ekphrasis docs\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user','time', 'url', 'date', 'number'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "              'emphasis', 'censored'},\n",
        "    segmenter=\"twitter\",\n",
        "    corrector=\"twitter\",\n",
        "    unpack_hashtags=True,  \n",
        "    unpack_contractions=True,  \n",
        "    spell_correct_elong=True,  \n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    dicts=[emoticons,negative_cont]\n",
        ")"
      ],
      "metadata": {
        "id": "haO9dh5PkHMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the above-mentioned preprocessor\n",
        "def clean_processor(text) : \n",
        "    text = \" \".join(text_processor.pre_process_doc(text))\n",
        "    return text\n",
        "\n",
        "stop = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Removing the stopwords\n",
        "def remove_stopwords(text):\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "f-EUGDn4kY6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spell corrector \n",
        "corrector = jamspell.TSpellCorrector()\n",
        "corrector.LoadLangModel('en.bin')"
      ],
      "metadata": {
        "id": "erCRMnkYyaRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows the tweets before and after spell correction\n",
        "def spell_tweet(tweet, debug=False):\n",
        "    if debug:\n",
        "        print(\"before spellcheck:\", tweet)\n",
        "    tweet = corrector.FixFragment(tweet)\n",
        "    if debug:\n",
        "        print(\"after check: \",tweet)\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "p-n5Vqtu0vsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean tweet"
      ],
      "metadata": {
        "id": "1LtLFwexpQhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet, rm_stopwords=True):\n",
        "    # lower case \n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # remove more than 2 char repetitions\n",
        "    tweet = handle_elong(tweet)\n",
        "\n",
        "    # remove emoticons\n",
        "    tweet = handle_emoticons(tweet)\n",
        "\n",
        "    #apply pre cleaning from ekphrasis - removes slang\n",
        "    tweet = rm_slang(tweet)\n",
        "    tweet = rm_extra_slang(tweet)\n",
        "\n",
        "    # process other tags left in the tweets - hashtags, emphasis, reps, contractions\n",
        "    tweet = clean_processor(tweet)    \n",
        "    #unpack slang words\n",
        "    \n",
        "    #replace ! and ? by \"exclamation\" and \"question\" resp\n",
        "    tweet = replace_exclamation(tweet)\n",
        "    tweet = replace_question(tweet)\n",
        "\n",
        "    # replace any number by \"number\"\n",
        "    tweet = replace_numbers(tweet)\n",
        "\n",
        "    # Spellcheck tweet\n",
        "    tweet = spell_tweet(tweet)\n",
        "\n",
        "    # remove remaining tags\n",
        "    tweet = re.sub('<\\/*\\w*\\/*>', \"\", tweet)\n",
        "\n",
        "    #remove all punctuation left\n",
        "    tweet = remove_punct(tweet)\n",
        "\n",
        "    # Replace negations and append them to the next token\n",
        "    # I did not like -> I did not_like\n",
        "    tweet = replace_negations(tweet)\n",
        "\n",
        "    \n",
        "    # lemmatize tweet\n",
        "    tweet = word_tokenize(tweet)\n",
        "    \n",
        "    tweet = ' '.join(word for word in tweet)\n",
        "\n",
        "    # Remove stopwords - it's set to True\n",
        "    if rm_stopwords:\n",
        "        tweet = remove_stopwords(tweet)\n",
        "\n",
        "    tweet = remove_tags(tweet)\n",
        "    return tweet "
      ],
      "metadata": {
        "id": "EXXWSuWKklwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for applying the stemmatizer/lemmatizer function down below\n",
        "def to_func(i, f):\n",
        "    r = f(word_tokenize(i))\n",
        "    r = \" \".join(r)\n",
        "    return r"
      ],
      "metadata": {
        "id": "MQQ3-5xWda1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data"
      ],
      "metadata": {
        "id": "-e4XdDHupWWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading labled tweets"
      ],
      "metadata": {
        "id": "CP5N68eupmbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname_tweets = \"orig_tweets_full.csv\"\n",
        "tweets_full = pd.read_csv(fname_tweets, sep=\";\")\n",
        "\n",
        "tweets_full = tweets_full.drop_duplicates(subset=['tweet']) # remove duplicates\n",
        "\n",
        "tweets_full['tweet_orig'] = tweets_full['tweet'] # copy for sanity"
      ],
      "metadata": {
        "id": "aM4S1JC3pwSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating datasets"
      ],
      "metadata": {
        "id": "xHsXWEYtrc8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the tweets_clean_full_min.csv used for training"
      ],
      "metadata": {
        "id": "-0QcByPfrfuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_full['tweet'] =  tweets_full['tweet_orig'].progress_apply(lambda x: clean_tweet(x))\n",
        "tweets_full['tweet_clean'] =  tweets_full['tweet']\n",
        "\n",
        "tweets_full['tweet_stem'] = tweets_full['tweet'].progress_apply(lambda x: to_func(x, stemmatizer))\n",
        "\n",
        "tweets_full['tweet_lemma'] = tweets_full['tweet'].progress_apply(lambda x: to_func(x, lemmatizer))"
      ],
      "metadata": {
        "id": "p374uKwgsWrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting sentiments from 1 and -1 to 1 and 0 since some models needs increasing classes starting from 0\n",
        "tweets_full['sentiment'] = tweets_full['sentiment'].apply(lambda x: 1 if x == 1 else 0)"
      ],
      "metadata": {
        "id": "b39i7eYbwLmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing empty tweets\n",
        "tweets_full.dropna(subset=['tweet'], inplace=True)"
      ],
      "metadata": {
        "id": "hvQcUKHLwWMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_full.to_csv('tweets_clean.tmp', sep=\";\", index=False)"
      ],
      "metadata": {
        "id": "XuEn_J8AcWdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty tweets after the cleanup\n",
        "td = pd.read_csv(\"tweets_clean.tmp\", sep=';')\n",
        "td.dropna(subset=['tweet'], inplace=True)"
      ],
      "metadata": {
        "id": "GX0Ekq8jw5oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving tweets\")\n",
        "td.to_csv(\"tweets_clean_full.csv\", sep=';', index=False)\n",
        "print(\"Done\")\n",
        "\n",
        "print(\"Saving mimimized file\")\n",
        "td = td[['tweet', 'sentiment']]\n",
        "td.to_csv(\"tweets_clean_full_min.csv\", sep=';', index=False)\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "TnmkTUioxBYy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}